# -*- coding: utf-8 -*-
"""YiselMNoa_HW2_6461.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iRi0cIe5qOY8PMQ88tDADuLvRLy1GvLe

1.- Write a python notebook that performs the following processes:
A) Given a number N, calculate N! using two different methods:
i. Write your own small program to multiply number from 1 to N and print N!:
"""

def factorial(N):
    start = 1
    for k in range(N, 0, -1):
        start = start * k
    return start


print(factorial(5))

"""Use a python function (there are several, try numpy for sure) to compute N!"""

import numpy as np


def factor(N):
    return np.math.factorial(N)


print(factor(5))

"""1.-B) Using one of the methods derived above, compute and plot N! vs N, with axes labelled."""

import numpy as np
import matplotlib.pyplot as plt


def factor(n):
    return np.math.factorial(n)


X = np.arange(1, 16, 1)

Y = np.array([factor(n) for n in X])

plt.plot(X, Y)
plt.xlabel('N')
plt.ylabel('N!')
# plt.savefig('N-factorial_vs_N.jpg')
plt.show()

"""1.-C) How large can N get in python so the N! can be computed? If there is a limit, why is it there?
Answer/ The limit is 170 since this is the largest integer that can be stored in IEEE 754 double-precision floating-point format (Double-precision binary floating-point is a commonly used format on PCs, due to its wider range over single-precision floating point, in spite of its performance and bandwidth cost). (Obtained from: https://en.wikipedia.org/wiki/170_(number))
"""

import numpy as np


def factor(N):
    return np.math.factorial(N)


print(factor(170))

"""1.-D) Using one of the methods derived in part A, compute and plot ln(N!) vs N, with axes
labelled.
"""

import numpy as np
import matplotlib.pyplot as plt


def fact(n):
    return np.math.factorial(n)


def ln(x):
    return np.log(x)


# print(ln(fact(5)))

X = np.arange(1, 6, 1)

Y = np.array([ln(fact(x)) for x in X])

plt.plot(X, Y)
plt.xlabel('N')
plt.ylabel('ln(N!)')
#plt.savefig('lnN-fact_vs_N.jpg')
plt.show()

"""1.-E) Use the Stirling approximation to compute and plot ln N! vs N"""

import numpy as np
import matplotlib.pyplot as plt


def fact(N):
    return np.math.factorial(N)


def ln(x):
    return np.log(x)


def strlg(x):
    return x * ln(x) - x


print(strlg(5))

X = np.arange(1, 21, 1)

Y1 = np.array([strlg(x) for x in X])

plt.plot(X, Y1)
plt.xlabel('N')
plt.ylabel('ln(N!)')
plt.legend(['Stirling Approximation'])
# plt.savefig('Stirling.jpg')
plt.show()

"""1.-F) Create a plot showing ln N! vs N computed directly (part D) and approximated via part E.
Add labels for the different lines.
"""

import numpy as np
import matplotlib.pyplot as plt


def fact(N):
    return np.math.factorial(N)

def ln(x):
    return np.log(x)


def strlg(x):
    return x * ln(x) - x


X = np.arange(1, 21, 1)

Y1 = np.array([strlg(x) for x in X])


def ln_fact(x):
    return ln(fact(x))

Y2 = np.array([ln_fact(x) for x in X])

plt.plot(X, Y1)
plt.plot(X, Y2)
plt.xlabel('N')
plt.ylabel('ln(N!)')
plt.legend(['Stirling Approximation', 'ln(N!) directly calculated'])
# plt.savefig('Stirling_vs_classical.jpg')
plt.show()

"""1.-G) Create a plot showing (ln N! – Stirling_approx(N!)/ ln N! to look at the relative error when
using the approximation.
"""

import numpy as np
import matplotlib.pyplot as plt


def fact(N):
    return np.math.factorial(N)



def ln(x):
    return np.log(x)



def strlg(x):
    return x * ln(x) - x


X = np.arange(1, 21, 1)

Y1 = np.array([strlg(x) for x in X])


def ln_fact(x):
    return ln(fact(x))


Y2 = np.array([ln_fact(x) for x in X])


def err(x):
    return (ln_fact(x) - strlg(x)) / ln_fact(x)


Y3 = np.array([err(x) for x in X])
plt.plot(X, Y3)
plt.xlabel('N')
plt.ylabel('Errors')
# plt.savefig('errors.jpg')
plt.show()

"""2.- A)  Plot the probability density for a normalized Gaussian Distribution with average zero and
standard deviation = 1
"""

import numpy as np
import matplotlib.pyplot as plt


def gauss(x, sigma=1, mu=0):
    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)


g = gauss(0, sigma=1)
print(g)

X = np.arange(-10, 10.01, 0.01)

Y = np.array([gauss(x) for x in X])

plt.plot(X, Y)
plt.xlabel('x')
plt.ylabel('Normalized Gaussian Distribution')
#plt.savefig('Gaussian.jpg')
plt.show()

"""2.- B) Redo for sigma 0.1, 0.01, 0.001"""

import numpy as np
import matplotlib.pyplot as plt
def gauss(x, sigma=1, mu=0):
    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)


g = gauss(0, sigma=1)

X = np.arange(-1, 1.01, 0.001)

sigma_parameters = [0.1, 0.01, 0.001]
legend = []
for sigma in sigma_parameters:
    Y = np.array([gauss(x, sigma=sigma) for x in X])
    plt.plot(X, Y)
    legend.append(str(sigma))
plt.xlabel('x')
plt.ylabel('Normalized Gaussian Distribution')
plt.legend(legend)
#plt.savefig('Gaussian_001-01-1.jpg')
plt.show()

"""2.-C) What happens to these distributions? Explain with words.

Answer/ As sigma tends to zero, the distribution curve becomes narrower, and increases at the same time, since the area under the curve must remain the independently of the sigma value.

2.- D) As sigma → 0, the distribution resembles a useful function in quantum mechanics. What
is that function called and give an example of how is it used ?

Answer/ In this case, the Gaussian turns into Dirac delta function. It is useful for generalizing results from discrete to continuous variables,  hence, it's known as a generalization of the Kronecker delta.
"""